{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 158 .adoc files\n",
      "Found 16 .json files\n",
      "Found 1 .yaml files\n",
      "Found 18 .yml files\n",
      "Found 158 .adoc files\n",
      "✅ Extracted docs exported to 'cell_2_extracted_adoc_docs.json'\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 Extract documentation texts\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize \n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=-1)\n",
    "DOC_TYPE_LABELS = [\"concept\", \"task\", \"reference\", \"overview\", \"tutorial\"]\n",
    "PERSONA_LABELS = [\"API Developer\", \"Storage Administrator\", \"DevOps Engineer\", \"Systems Administrator\", \"IT Director\"]\n",
    "\n",
    "\n",
    "def extract_texts(repo_path, extensions=(\"adoc\", \"json\", \"yaml\", \"yml\")):\n",
    "    \"\"\"\n",
    "    Extract texts from files with specified extensions in the given repository path.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    # Fix: Process all extensions, not just the first one\n",
    "    for ext in extensions:\n",
    "        file_paths = list(Path(repo_path).rglob(f\"*.{ext}\"))\n",
    "        print(f\"Found {len(file_paths)} .{ext} files\")\n",
    "        \n",
    "        for path in file_paths:\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    if content.strip():  # Only add non-empty files\n",
    "                        texts.append((str(path), content, ext))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def extract_adoc_texts(repo_path):\n",
    "    \"\"\"\n",
    "    Extract texts specifically from .adoc files in the given repository path.\n",
    "    \"\"\"\n",
    "    adoc_texts = []\n",
    "    file_paths = list(Path(repo_path).rglob(\"*.adoc\"))\n",
    "    print(f\"Found {len(file_paths)} .adoc files\")\n",
    "    \n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                if content.strip():  # Only add non-empty files\n",
    "                    adoc_texts.append((str(path), content, 'adoc'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return adoc_texts\n",
    "\n",
    "def docs_to_json(docs, filename):\n",
    "    # Export extracted docs to a JSON file\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(docs, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Extracted docs exported to '{filename}.json'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting extracted '{filename}.json' docs: {e}\")\n",
    "\n",
    "docs = extract_texts(\"../data/bluexp-dataset\")\n",
    "#print(f\"Extracted {len(docs)} files\")\n",
    "\n",
    "#docs_to_json(docs, \"cell_2_extracted_docs\")\n",
    "docs_to_json( extract_adoc_texts(\"../data/bluexp-dataset\"), \"cell_2_extracted_adoc_docs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 metadata \n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import hashlib\n",
    "import uuid\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#CSV Header\n",
    "# file_name (url)\n",
    "# file_ext (json, adoc, etc.)\n",
    "# uuid\n",
    "# repo_name\n",
    "# last_modified\n",
    "# file_purpose_use (\"task, concept\")\n",
    "# summary (purpose)\n",
    "# chapter\n",
    "# keywords\n",
    "# persona (business,developer,system admin, overweight)\n",
    "# synonyms\n",
    "# section heading\n",
    "\n",
    "\n",
    "# Extracted\n",
    "def split_frontmatter(text):\n",
    "    front, body = {}, text\n",
    "    if text.startswith('---'):\n",
    "        parts = text.split('---', 2)\n",
    "        if len(parts) >= 3:\n",
    "            front = yaml.safe_load(parts[1]) or {}\n",
    "            body = parts[2]\n",
    "    return front, body\n",
    "\n",
    "# Only for .adoc bodies\n",
    "def extract_title(body):\n",
    "    for line in body.splitlines():\n",
    "        if line.startswith('= '):\n",
    "            return line.lstrip('= ').strip()\n",
    "    return None\n",
    "\n",
    "def extract_headings(body, level=2):\n",
    "    return re.findall(rf\"^{'='*level}\\s+(.*)\", body, flags=re.MULTILINE)\n",
    "\n",
    "def extract_prerequisites(body):\n",
    "    out, cap = [], False\n",
    "    for line in body.splitlines():\n",
    "        if re.match(r\"^==+\\s+Prerequisites\", line): cap = True; continue\n",
    "        if cap:\n",
    "            if re.match(r\"^==+\\s+\", line): break\n",
    "            if line.strip(): out.append(line.strip('-* '))\n",
    "    return out\n",
    "\n",
    "def estimate_reading_time(body, wpm=200):\n",
    "    return max(1, len(re.findall(r\"\\w+\", body)) // wpm)\n",
    "\n",
    "def extract_api_endpoints(body):\n",
    "    return [f\"{m} {p}\" for m, p in re.findall(r\"\\b(GET|POST|PUT|DELETE|PATCH)\\s+(/[\\w_{}\\-\\[\\]/]+)\", body)]\n",
    "    \n",
    "\n",
    "def extract_tags_tfidf(corpus, top_n=5):\n",
    "    idxs = [i for i, doc in enumerate(corpus) if re.search(r\"\\w+\", doc)]\n",
    "    tags = [[] for _ in corpus]\n",
    "    if not idxs: return tags\n",
    "    filtered = [corpus[i] for i in idxs]\n",
    "    vect = TfidfVectorizer(max_df=0.8, stop_words='english', ngram_range=(1,2))\n",
    "    X = vect.fit_transform(filtered)\n",
    "    feats = vect.get_feature_names_out()\n",
    "    for i, orig in enumerate(idxs):\n",
    "        arr = X[i].toarray().flatten()\n",
    "        top = arr.argsort()[::-1][:top_n]\n",
    "        tags[orig] = [feats[j] for j in top]\n",
    "    return tags\n",
    "\n",
    "#Summarizer\n",
    "def summarize_purpose(text):\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(f\"Expected a string, but got {type(text)}\")\n",
    "\n",
    "    # Extract a snippet of the text\n",
    "    snippet = text[:1000]\n",
    "\n",
    "    # Call the summarizer\n",
    "    out = summarizer(snippet, max_length=60, min_length=10, do_sample=False)\n",
    "\n",
    "    # Return the summary text\n",
    "    return out[0]['summary_text'].strip()\n",
    "\n",
    "# Extract section headings\n",
    "def extract_section_headings(content):\n",
    "    \"\"\"\n",
    "    Extracts section headings from the given content based on a specific pattern.\n",
    "\n",
    "    This function identifies lines in the content that start with one or more \n",
    "    '=' characters followed by a space and a title. It determines the level of \n",
    "    the heading based on the number of '=' characters and returns a list of \n",
    "    formatted headings.\n",
    "\n",
    "    # AsciiDoc Heading Syntax\n",
    "    = Document Title → Level 0 (already extracted as title)\n",
    "\n",
    "    == Section A → Level 1\n",
    "\n",
    "    === Subsection A.1 → Level 2\n",
    "\n",
    "    ==== Sub-subsection A.1.1 → Level 3\n",
    "\n",
    "    Args:\n",
    "        content (str): The input text content from which section headings \n",
    "                       are to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing the extracted section headings, \n",
    "              formatted as \"Level {level}: {title}\" where {level} is the \n",
    "              number of '=' characters and {title} is the heading text.\n",
    "    \"\"\"\n",
    "    headings = []\n",
    "    for line in content.splitlines():\n",
    "        match = re.match(r\"^(==+)\\s+(.*)\", line.strip())\n",
    "        if match:\n",
    "            level = len(match.group(1))  # Count of '=' characters\n",
    "            title = match.group(2).strip()\n",
    "            headings.append(f\"Level {level}: {title}\")\n",
    "    return headings\n",
    "\n",
    "def get_last_modified_datetime(file_path):\n",
    "    \"\"\"\n",
    "    Get the last modified datetime of a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        last_modified_timestamp = os.path.getmtime(file_path)\n",
    "        return datetime.fromtimestamp(last_modified_timestamp)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving last modified datetime for {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Synonym expansion using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    seen = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if (\n",
    "                synonym != word.lower()\n",
    "                and synonym not in seen\n",
    "                and len(synonym) > 2\n",
    "            ):\n",
    "                seen.add(synonym)\n",
    "                synonyms.append(synonym)\n",
    "            if len(synonyms) == 3:\n",
    "                return synonyms\n",
    "    return synonyms\n",
    "\n",
    "\n",
    "def generate_synonym_data(title, keywords, section_headings):\n",
    "    \"\"\"\n",
    "    Best fit sources for synonym_data extraction:\n",
    "\n",
    "    keywords: main base\n",
    "    title: most direct paraphrasable noun-verb pair\n",
    "    section_headings: reinforce and diversify search terms\n",
    "    \"\"\"\n",
    "    base_terms = set(keywords + [title] + section_headings)\n",
    "    enriched = []\n",
    "    for term in base_terms:\n",
    "        # naive synonym example\n",
    "        if \"create\" in term.lower():\n",
    "            enriched.append(\"generate\")\n",
    "        if \"token\" in term.lower():\n",
    "            enriched.append(\"access key\")\n",
    "        # ... add more rules\n",
    "    return list(set(enriched))\n",
    "\n",
    "\n",
    "def generate_uuid(path: str) -> uuid.UUID:\n",
    "    with open(path, 'rb') as f:\n",
    "        content = f.read()\n",
    "    digest = hashlib.sha256(content).hexdigest()\n",
    "    return uuid.uuid5(uuid.NAMESPACE_URL, digest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 60, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 60, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 60, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 60, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 60, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 60, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 60, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 60, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 60, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 60, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 60, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 60, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 60, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 60, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 60, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 60, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
      "Your max_length is set to 60, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 60, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "#CSV Header\n",
    "# file_name (url)\n",
    "# file_ext (json, adoc, etc.)\n",
    "# uuid\n",
    "# repo_name\n",
    "# last_modified\n",
    "# file_purpose_use (\"task, concept\")\n",
    "# summary (purpose) Using facebook/bart-large-cnn\n",
    "# doc_type (concept, task, reference, overview, tutorial) Using facebook/bart-large-mnli\n",
    "# chapter TBD\n",
    "# keywords\n",
    "# persona (business,developer,system admin, overweight)\n",
    "# synonyms (title, keywords, section headings) using NLTK WordNet a lexical database for the English language\n",
    "# section heading (subheadings of adoc files)\n",
    "def extract_metedata_from_docs(docs):\n",
    "    \"\"\"\n",
    "    Extract metadata from the documentation texts.\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = [split_frontmatter(content)[1] if ext=='adoc' else content for _, content, ext in docs]\n",
    "    tag_lists = extract_tags_tfidf(corpus)\n",
    "\n",
    "    metadata_list = []\n",
    "    for entry in docs:\n",
    "        path, content, fmt = entry\n",
    "        metadata = {}\n",
    "        metadata[\"file_path\"] = path\n",
    "        metadata[\"file_name\"] = Path(path).name  \n",
    "        metadata[\"repo_name\"] = Path(path).parts[2]  # Assuming the second part is the repo name\n",
    "        metadata[\"ext\"] = fmt\n",
    "\n",
    "        body = split_frontmatter(content) if fmt=='adoc' else ({}, content)\n",
    "\n",
    "        # Extract title from content\n",
    "        lines = content.splitlines()\n",
    "        title = next((line for line in lines if line.startswith(\"= \")), \"\").replace(\"= \", \"\")\n",
    "        metadata[\"title\"] = title.strip()\n",
    "\n",
    "        # Extract UUID\n",
    "        uuid_line = next((line for line in lines if \"uuid:\" in line), \"\")\n",
    "        metadata[\"uuid\"] = uuid_line.split(\"uuid:\")[-1].strip() if \"uuid:\" in uuid_line else None\n",
    "\n",
    "        # Extract summary\n",
    "        summary_line = next((line for line in lines if \"summary:\" in line), \"\")\n",
    "        metadata[\"summary\"] = summary_line.split(\"summary:\")[-1].strip() if \"summary:\" in summary_line else None\n",
    "\n",
    "        # Extract keywords\n",
    "        keywords_line = next((line for line in lines if \"keywords:\" in line), \"\")\n",
    "        metadata[\"keywords\"] = [kw.strip() for kw in keywords_line.split(\"keywords:\")[-1].split(\",\")] if \"keywords:\" in keywords_line else []   \n",
    "\n",
    "        # Extract persona\n",
    "        metadata[\"persona\"] = classifier(summarize_purpose(content), PERSONA_LABELS)['labels'][0] #add frequency penalty , mini shot classification\n",
    "\n",
    "        # Extract persona\n",
    "        metadata[\"doc_type\"] = classifier(summarize_purpose(content), DOC_TYPE_LABELS)['labels'][0]\n",
    "        # Add last modified timestamp (convert datetime to string)\n",
    "        last_modified = get_last_modified_datetime(path)\n",
    "        metadata[\"last_modified\"] = last_modified.isoformat() if last_modified else None\n",
    "        metadata[\"section_headings\"] = extract_section_headings(content)\n",
    "\n",
    "        # Collect unique terms from title, keywords, and section headings\n",
    "        conjunctions = {\"and\", \"or\", \"so\", \"for\", \"user\"}\n",
    "        base_terms = set(word.lower() for word in metadata[\"title\"].split() if len(word) > 2 and word.lower() not in conjunctions)\n",
    "        base_terms.update([kw.strip().lower() for kw in metadata[\"keywords\"] if len(kw.strip()) > 2 and kw.strip().lower() not in conjunctions])\n",
    "        for heading in metadata[\"section_headings\"]:\n",
    "            words = re.findall(r\"\\b\\w+\\b\", heading.lower())\n",
    "            base_terms.update(word for word in words if len(word) > 2 and word not in conjunctions)\n",
    "        \n",
    "        # Create synonym data string\n",
    "        synonym_data = \", \".join({synonym for term in base_terms if term.isalpha() for synonym in get_synonyms(term)})\n",
    "        metadata[\"synonym_data\"] = synonym_data\n",
    "        metadata[\"tags\"] = tag_lists[docs.index(entry)] if docs.index(entry) < len(tag_lists) else []\n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    return metadata_list\n",
    "\n",
    "# Use the existing 'docs' variable from Cell 2\n",
    "\n",
    "metadata_results = extract_metedata_from_docs(docs)\n",
    "\n",
    "# Convert metadata_results to CSV format\n",
    "docs_to_json(metadata_results, \"cell_3_extracted_metadata.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted docs exported to 'cell_3_extracted_metadata.json.json'\n",
      "Metadata exported to 'cell_3_extracted_metadata.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Update CSV headers to include missing fields\n",
    "csv_headers = [\n",
    "    \"file_path\", \"file_name\", \"repo_name\", \"ext\", \"title\", \"uuid\", \"summary\",\n",
    "    \"keywords\", \"last_modified\", \"section_headings\", \"synonym_data\", \"tags\",\n",
    "    \"persona\", \"doc_type\"\n",
    "]\n",
    "\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = \"metadata_output.csv\"\n",
    "\n",
    "with open(csv_file_path, mode=\"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "    for metadata in metadata_results:\n",
    "        # Flatten lists for CSV compatibility\n",
    "        metadata[\"keywords\"] = \", \".join(metadata[\"keywords\"])\n",
    "        metadata[\"section_headings\"] = \", \".join(metadata[\"section_headings\"])\n",
    "        metadata[\"tags\"] = \", \".join(metadata[\"tags\"])\n",
    "        writer.writerow(metadata)\n",
    "\n",
    "print(f\"Metadata exported to '{csv_file_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
