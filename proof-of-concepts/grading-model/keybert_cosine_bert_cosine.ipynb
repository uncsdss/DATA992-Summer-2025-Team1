{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772d970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, re, ast\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31318c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONFIG \n",
    "ADOC_DIR = \"bluexp-automation-main\"\n",
    "TEST_CSV = \"bluexp_test_queries.csv\"\n",
    "RESULT_CSV = \"bluexp_query_evaluation_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d3295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 1: Load .adoc Files \n",
    "adoc_files = []\n",
    "for root, dirs, files in os.walk(ADOC_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith(\".adoc\"):\n",
    "            path = os.path.join(root, file)\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                adoc_files.append({\n",
    "                    \"filename\": file,\n",
    "                    \"filepath\": path,\n",
    "                    \"content\": f.read()\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55e39c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved test queries to: bluexp_test_queries.csv\n"
     ]
    }
   ],
   "source": [
    "#Test set\n",
    "testset = []\n",
    "for file in adoc_files:\n",
    "    query = os.path.splitext(file[\"filename\"])[0].replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    permalink_match = re.search(r\"permalink:\\s*(.+)\", file[\"content\"])\n",
    "    if permalink_match:\n",
    "        url = f\"https://docs.netapp.com/us-en/bluexp-automation/{permalink_match.group(1).strip()}\"\n",
    "        testset.append({\"query\": query, \"expected_url\": url})\n",
    "\n",
    "\n",
    "# Save test queries\n",
    "testset_df = pd.DataFrame(testset)\n",
    "testset_df.to_csv(TEST_CSV, index=False)\n",
    "print(f\"‚úÖ Saved test queries to: {TEST_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790bc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 2: Enrich Metadata Embed .adoc Files \n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model=embed_model)\n",
    "\n",
    "docs = []\n",
    "for file in adoc_files:\n",
    "    content = file[\"content\"]\n",
    "    keywords = kw_model.extract_keywords(content, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)\n",
    "    topic_tags = [kw[0] for kw in keywords]\n",
    "    embedding = embed_model.encode(content, convert_to_tensor=True)\n",
    "    permalink_match = re.search(r\"permalink:\\s*(.+)\", content)\n",
    "    doc_url = f\"https://docs.netapp.com/us-en/bluexp-automation/{permalink_match.group(1).strip()}\" if permalink_match else None\n",
    "\n",
    "    docs.append({\n",
    "        \"filename\": file[\"filename\"],\n",
    "        \"doc_url\": doc_url,\n",
    "        \"topic_tags\": \", \".join(topic_tags),\n",
    "        \"embedding\": embedding\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb694b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 3: Semantic Matching \n",
    "def find_best_match(query, docs, top_n=1):\n",
    "    query_embedding = embed_model.encode(query, convert_to_tensor=True)\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        score = util.cos_sim(query_embedding, doc[\"embedding\"]).item()\n",
    "        results.append({\n",
    "            \"filename\": doc[\"filename\"],\n",
    "            \"doc_url\": doc[\"doc_url\"],\n",
    "            \"topic_tags\": doc[\"topic_tags\"],\n",
    "            \"similarity\": round(score, 4)\n",
    "        })\n",
    "    return pd.DataFrame(results).sort_values(by=\"similarity\", ascending=False).head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94bfcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_df = pd.read_csv(TEST_CSV)\n",
    "test_query_df[\"keywords\"] = test_query_df[\"query\"].apply(lambda x: x.split())\n",
    "test_query_df[\"urls\"] = test_query_df[\"expected_url\"].apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e02b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 5: Evaluate All Test Cases \n",
    "evaluation_results = []\n",
    "\n",
    "for _, row in test_query_df.iterrows():\n",
    "    query_text = \" \".join(row[\"keywords\"])\n",
    "    expected_urls = row[\"urls\"]\n",
    "\n",
    "    top_match = find_best_match(query_text, docs, top_n=1)\n",
    "    predicted_url = top_match.iloc[0][\"doc_url\"] if not top_match.empty else None\n",
    "    match = predicted_url in expected_urls\n",
    "\n",
    "    evaluation_results.append({\n",
    "        \"query\": query_text,\n",
    "        \"expected_urls\": expected_urls,\n",
    "        \"predicted_url\": predicted_url,\n",
    "        \"match\": match,\n",
    "        \"similarity\": top_match.iloc[0][\"similarity\"] if not top_match.empty else 0\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e16452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    query                                      expected_urls  \\\n",
      "0           legal notices  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "1                   blogs  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "2                overview  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "3           api reference  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "4     api ref definitions  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "..                    ...                                                ...   \n",
      "153      register service  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "154    user access tokens  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "155         use rest apis  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "156       workflows tasks  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "157  additional resources  [https://docs.netapp.com/us-en/bluexp-automati...   \n",
      "\n",
      "                                         predicted_url  match  similarity  \n",
      "0    https://docs.netapp.com/us-en/bluexp-automatio...   True      0.5610  \n",
      "1    https://docs.netapp.com/us-en/bluexp-automatio...  False      0.2009  \n",
      "2    https://docs.netapp.com/us-en/bluexp-automatio...  False      0.2403  \n",
      "3    https://docs.netapp.com/us-en/bluexp-automatio...   True      0.5070  \n",
      "4    https://docs.netapp.com/us-en/bluexp-automatio...   True      0.4674  \n",
      "..                                                 ...    ...         ...  \n",
      "153  https://docs.netapp.com/us-en/bluexp-automatio...   True      0.4110  \n",
      "154  https://docs.netapp.com/us-en/bluexp-automatio...   True      0.6317  \n",
      "155  https://docs.netapp.com/us-en/bluexp-automatio...  False      0.4932  \n",
      "156  https://docs.netapp.com/us-en/bluexp-automatio...   True      0.4496  \n",
      "157  https://docs.netapp.com/us-en/bluexp-automatio...   True      0.3563  \n",
      "\n",
      "[158 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#  STEP 6: Save or Print Results \n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "print(results_df)\n",
    "results_df.to_csv(\"query_evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d14540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Original Accuracy: 74.68%\n",
      "üü¢ New Accuracy with Semantic Tags: 81.65%\n",
      "\n",
      "‚úÖ Corrected Matches:\n",
      "                              query  \\\n",
      "12  wf aws cloud create we capacity   \n",
      "34           wf aws ontap get aggrs   \n",
      "35            wf aws ontap get cifs   \n",
      "37         wf aws ontap get volumes   \n",
      "44           wf azure cloud get wes   \n",
      "\n",
      "                                        predicted_url  new_similarity  \n",
      "12  https://docs.netapp.com/us-en/bluexp-automatio...        0.692443  \n",
      "34  https://docs.netapp.com/us-en/bluexp-automatio...        0.652191  \n",
      "35  https://docs.netapp.com/us-en/bluexp-automatio...        0.573620  \n",
      "37  https://docs.netapp.com/us-en/bluexp-automatio...        0.578940  \n",
      "44  https://docs.netapp.com/us-en/bluexp-automatio...        0.538763  \n",
      "\n",
      "‚ö†Ô∏è New Misses:\n",
      "                query                                      predicted_url  \\\n",
      "3       api reference  https://docs.netapp.com/us-en/bluexp-automatio...   \n",
      "8             prepare  https://docs.netapp.com/us-en/bluexp-automatio...   \n",
      "67   wf common before  https://docs.netapp.com/us-en/bluexp-automatio...   \n",
      "141       aa concepts  https://docs.netapp.com/us-en/bluexp-automatio...   \n",
      "143      api explorer  https://docs.netapp.com/us-en/bluexp-automatio...   \n",
      "\n",
      "     new_similarity  \n",
      "3          0.320878  \n",
      "8          0.284319  \n",
      "67         0.313076  \n",
      "141        0.311574  \n",
      "143        0.437140  \n"
     ]
    }
   ],
   "source": [
    "# Step 7: Enrich Queries with Semantic Tags and Recalculate Accuracy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load the CSV file (update path if needed)\n",
    "results_df = pd.read_csv(\"query_evaluation_results.csv\")\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Simulated LLM-style tag extractor\n",
    "def generate_semantic_tags(query):\n",
    "    keywords = re.findall(\n",
    "        r'\\b(create|get|delete|update|volume|buckets|wes|ontap|gcp|aws|azure|onprem|add|remove|modify|snapshot|replication|kubernetes)\\b',\n",
    "        query.lower()\n",
    "    )\n",
    "    return list(set(keywords))\n",
    "\n",
    "# Enrich each query\n",
    "def enrich_query(query):\n",
    "    tags = generate_semantic_tags(query)\n",
    "    tag_str = \" \".join(tags)\n",
    "    return f\"{query} [TAGS: {tag_str}]\"\n",
    "\n",
    "results_df['enriched_query'] = results_df['query'].apply(enrich_query)\n",
    "\n",
    "# Embed enriched queries and predicted_url (treat as proxy for page text)\n",
    "query_embeddings = model.encode(results_df['enriched_query'].tolist(), convert_to_tensor=False)\n",
    "doc_embeddings = model.encode(results_df['predicted_url'].astype(str).tolist(), convert_to_tensor=False)\n",
    "\n",
    "# Cosine similarity and new match logic\n",
    "similarities = cosine_similarity(query_embeddings, doc_embeddings).diagonal()\n",
    "results_df['new_similarity'] = similarities\n",
    "\n",
    "# Threshold can be adjusted ‚Äî start with 0.45\n",
    "threshold = 0.45\n",
    "results_df['new_match'] = results_df['new_similarity'] > threshold\n",
    "\n",
    "# Accuracy comparison\n",
    "old_accuracy = results_df['match'].mean()\n",
    "new_accuracy = results_df['new_match'].mean()\n",
    "\n",
    "print(f\"üîµ Original Accuracy: {old_accuracy:.2%}\")\n",
    "print(f\"üü¢ New Accuracy with Semantic Tags: {new_accuracy:.2%}\")\n",
    "\n",
    "# Optional: See improved and worsened examples\n",
    "improved = results_df[(results_df['match'] == False) & (results_df['new_match'] == True)]\n",
    "worsened = results_df[(results_df['match'] == True) & (results_df['new_match'] == False)]\n",
    "\n",
    "print(\"\\n‚úÖ Corrected Matches:\")\n",
    "print(improved[['query', 'predicted_url', 'new_similarity']].head())\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è New Misses:\")\n",
    "print(worsened[['query', 'predicted_url', 'new_similarity']].head())\n",
    "\n",
    "# Save updated results if needed\n",
    "# results_df.to_csv(\"query_eval_enriched.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
