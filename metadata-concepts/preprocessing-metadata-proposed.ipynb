{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 158 .adoc files\n",
      "Found 16 .json files\n",
      "Found 1 .yaml files\n",
      "Found 18 .yml files\n",
      "Extracted 193 files\n",
      "Found 158 .adoc files\n",
      "✅ Extracted docs exported to 'cell_2_extracted_adoc_docs.json'\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 Extract documentation texts\n",
    "def extract_texts(repo_path, extensions=(\"adoc\", \"json\", \"yaml\", \"yml\")):\n",
    "    \"\"\"\n",
    "    Extract texts from files with specified extensions in the given repository path.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    # Fix: Process all extensions, not just the first one\n",
    "    for ext in extensions:\n",
    "        file_paths = list(Path(repo_path).rglob(f\"*.{ext}\"))\n",
    "        print(f\"Found {len(file_paths)} .{ext} files\")\n",
    "        \n",
    "        for path in file_paths:\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                    if content.strip():  # Only add non-empty files\n",
    "                        texts.append((str(path), content, ext))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def extract_adoc_texts(repo_path):\n",
    "    \"\"\"\n",
    "    Extract texts specifically from .adoc files in the given repository path.\n",
    "    \"\"\"\n",
    "    adoc_texts = []\n",
    "    file_paths = list(Path(repo_path).rglob(\"*.adoc\"))\n",
    "    print(f\"Found {len(file_paths)} .adoc files\")\n",
    "    \n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                if content.strip():  # Only add non-empty files\n",
    "                    adoc_texts.append((str(path), content, 'adoc'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return adoc_texts\n",
    "\n",
    "def docs_to_json(docs, filename):\n",
    "    # Export extracted docs to a JSON file\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(docs, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Extracted docs exported to '{filename}.json'\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting extracted '{filename}.json' docs: {e}\")\n",
    "\n",
    "docs = extract_texts(\"../data/bluexp-dataset\")\n",
    "print(f\"Extracted {len(docs)} files\")\n",
    "\n",
    "#docs_to_json(docs, \"cell_2_extracted_docs\")\n",
    "docs_to_json( extract_adoc_texts(\"../data/bluexp-dataset\"), \"cell_2_extracted_adoc_docs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted docs exported to 'cell_3_extracted_metadata.json.json'\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 metadata \n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "# Extract section headings\n",
    "def extract_section_headings(content):\n",
    "    \"\"\"\n",
    "    Extracts section headings from the given content based on a specific pattern.\n",
    "\n",
    "    This function identifies lines in the content that start with one or more \n",
    "    '=' characters followed by a space and a title. It determines the level of \n",
    "    the heading based on the number of '=' characters and returns a list of \n",
    "    formatted headings.\n",
    "\n",
    "    # AsciiDoc Heading Syntax\n",
    "    = Document Title → Level 0 (already extracted as title)\n",
    "\n",
    "    == Section A → Level 1\n",
    "\n",
    "    === Subsection A.1 → Level 2\n",
    "\n",
    "    ==== Sub-subsection A.1.1 → Level 3\n",
    "\n",
    "    Args:\n",
    "        content (str): The input text content from which section headings \n",
    "                       are to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing the extracted section headings, \n",
    "              formatted as \"Level {level}: {title}\" where {level} is the \n",
    "              number of '=' characters and {title} is the heading text.\n",
    "    \"\"\"\n",
    "    headings = []\n",
    "    for line in content.splitlines():\n",
    "        match = re.match(r\"^(==+)\\s+(.*)\", line.strip())\n",
    "        if match:\n",
    "            level = len(match.group(1))  # Count of '=' characters\n",
    "            title = match.group(2).strip()\n",
    "            headings.append(f\"Level {level}: {title}\")\n",
    "    return headings\n",
    "\n",
    "def get_last_modified_datetime(file_path):\n",
    "    \"\"\"\n",
    "    Get the last modified datetime of a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        last_modified_timestamp = os.path.getmtime(file_path)\n",
    "        return datetime.fromtimestamp(last_modified_timestamp)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving last modified datetime for {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Synonym expansion using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    seen = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if (\n",
    "                synonym != word.lower()\n",
    "                and synonym not in seen\n",
    "                and len(synonym) > 2\n",
    "            ):\n",
    "                seen.add(synonym)\n",
    "                synonyms.append(synonym)\n",
    "            if len(synonyms) == 3:\n",
    "                return synonyms\n",
    "    return synonyms\n",
    "\n",
    "\n",
    "def generate_synonym_data(title, keywords, section_headings):\n",
    "    \"\"\"\n",
    "    Best fit sources for synonym_data extraction:\n",
    "\n",
    "    keywords: main base\n",
    "    title: most direct paraphrasable noun-verb pair\n",
    "    section_headings: reinforce and diversify search terms\n",
    "    \"\"\"\n",
    "    base_terms = set(keywords + [title] + section_headings)\n",
    "    enriched = []\n",
    "    for term in base_terms:\n",
    "        # naive synonym example\n",
    "        if \"create\" in term.lower():\n",
    "            enriched.append(\"generate\")\n",
    "        if \"token\" in term.lower():\n",
    "            enriched.append(\"access key\")\n",
    "        # ... add more rules\n",
    "    return list(set(enriched))\n",
    "\n",
    "\n",
    "def extract_metedata_from_docs(docs):\n",
    "    \"\"\"\n",
    "    Extract metadata from the documentation texts.\n",
    "    \"\"\"\n",
    "    metadata_list = []\n",
    "    for entry in docs:\n",
    "        path, content, fmt = entry\n",
    "        metadata = {}\n",
    "        metadata[\"file_path\"] = path\n",
    "        metadata[\"format\"] = fmt\n",
    "        # Extract title from content\n",
    "        lines = content.splitlines()\n",
    "        title = next((line for line in lines if line.startswith(\"= \")), \"\").replace(\"= \", \"\")\n",
    "        metadata[\"title\"] = title.strip()\n",
    "\n",
    "        # Extract UUID\n",
    "        uuid_line = next((line for line in lines if \"uuid:\" in line), \"\")\n",
    "        metadata[\"uuid\"] = uuid_line.split(\"uuid:\")[-1].strip() if \"uuid:\" in uuid_line else None\n",
    "\n",
    "        # Extract summary\n",
    "        summary_line = next((line for line in lines if \"summary:\" in line), \"\")\n",
    "        metadata[\"summary\"] = summary_line.split(\"summary:\")[-1].strip() if \"summary:\" in summary_line else None\n",
    "\n",
    "        # Extract keywords\n",
    "        keywords_line = next((line for line in lines if \"keywords:\" in line), \"\")\n",
    "        metadata[\"keywords\"] = [kw.strip() for kw in keywords_line.split(\"keywords:\")[-1].split(\",\")] if \"keywords:\" in keywords_line else []   \n",
    "        \n",
    "        # Add last modified timestamp (convert datetime to string)\n",
    "        last_modified = get_last_modified_datetime(path)\n",
    "        metadata[\"last_modified\"] = last_modified.isoformat() if last_modified else None\n",
    "        metadata[\"section_headings\"] = extract_section_headings(content)\n",
    "\n",
    "        # Collect unique terms from title, keywords, and section headings\n",
    "        conjunctions = {\"and\", \"or\", \"so\", \"for\"}\n",
    "        base_terms = set(word.lower() for word in metadata[\"title\"].split() if len(word) > 2 and word.lower() not in conjunctions)\n",
    "        base_terms.update([kw.strip().lower() for kw in metadata[\"keywords\"] if len(kw.strip()) > 2 and kw.strip().lower() not in conjunctions])\n",
    "        for heading in metadata[\"section_headings\"]:\n",
    "            words = re.findall(r\"\\b\\w+\\b\", heading.lower())\n",
    "            base_terms.update(word for word in words if len(word) > 2 and word not in conjunctions)\n",
    "        # Create synonym data dictionary\n",
    "        synonym_data = {term: get_synonyms(term) for term in base_terms if term.isalpha()}\n",
    "        metadata[\"synonym_data\"] = synonym_data\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    return metadata_list\n",
    "# Use the existing 'docs' variable from Cell 2\n",
    "docs2 = docs\n",
    "\n",
    "\n",
    "metadata_results = extract_metedata_from_docs(docs2)\n",
    "docs_to_json(metadata_results, \"cell_3_extracted_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
