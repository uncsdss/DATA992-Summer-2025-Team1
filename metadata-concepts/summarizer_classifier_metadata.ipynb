{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba784def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fab44496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: c:\\Users\\huber\\netapp_team_1\\DATA992-Summer-2025-Team1\\metadata-concepts\n"
     ]
    }
   ],
   "source": [
    "print(\"cwd:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f02d77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huber\\netapp_team_1\\DATA992-Summer-2025-Team1\\data\\bluexp-dataset\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = Path.cwd().parent / \"data\" / \"bluexp-dataset\"\n",
    "\n",
    "EXTENSIONS = (\"adoc\", \"yaml\", \"yml\", \"json\")\n",
    "\n",
    "print(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b31eb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Config\n",
    "# ROOT_PATH = \"/data/bluexp-dataset\"\n",
    "# EXTENSIONS = (\"adoc\", \"yaml\", \"yml\", \"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf3ef4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize \n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=-1)\n",
    "DOC_TYPE_LABELS = [\"concept\", \"task\", \"reference\", \"overview\", \"tutorial\"]\n",
    "PERSONA_LABELS = [\"API Developer\", \"Storage Administrator\", \"DevOps Engineer\", \"Systems Administrator\", \"IT Director\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2588b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Extraction\n",
    "def extract_texts(repo_path, extensions=EXTENSIONS):\n",
    "    texts = []\n",
    "    for ext in extensions:\n",
    "        for path in Path(repo_path).rglob(f\"*.{ext}\"):\n",
    "            try:\n",
    "                content = path.read_text(encoding='utf-8', errors='ignore')\n",
    "                texts.append((str(path), content, ext))\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to read {path}: {e}\")\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74e7c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted\n",
    "def split_frontmatter(text):\n",
    "    front, body = {}, text\n",
    "    if text.startswith('---'):\n",
    "        parts = text.split('---', 2)\n",
    "        if len(parts) >= 3:\n",
    "            front = yaml.safe_load(parts[1]) or {}\n",
    "            body = parts[2]\n",
    "    return front, body\n",
    "\n",
    "# Only for .adoc bodies\n",
    "def extract_title(body):\n",
    "    for line in body.splitlines():\n",
    "        if line.startswith('= '):\n",
    "            return line.lstrip('= ').strip()\n",
    "    return None\n",
    "\n",
    "def extract_headings(body, level=2):\n",
    "    return re.findall(rf\"^{'='*level}\\s+(.*)\", body, flags=re.MULTILINE)\n",
    "\n",
    "def extract_prerequisites(body):\n",
    "    out, cap = [], False\n",
    "    for line in body.splitlines():\n",
    "        if re.match(r\"^==+\\s+Prerequisites\", line): cap = True; continue\n",
    "        if cap:\n",
    "            if re.match(r\"^==+\\s+\", line): break\n",
    "            if line.strip(): out.append(line.strip('-* '))\n",
    "    return out\n",
    "\n",
    "def estimate_reading_time(body, wpm=200):\n",
    "    return max(1, len(re.findall(r\"\\w+\", body)) // wpm)\n",
    "\n",
    "def extract_api_endpoints(body):\n",
    "    return [f\"{m} {p}\" for m, p in re.findall(r\"\\b(GET|POST|PUT|DELETE|PATCH)\\s+(/[\\w_{}\\-\\[\\]/]+)\", body)]\n",
    "    \n",
    "\n",
    "def extract_tags_tfidf(corpus, top_n=5):\n",
    "    idxs = [i for i, doc in enumerate(corpus) if re.search(r\"\\w+\", doc)]\n",
    "    tags = [[] for _ in corpus]\n",
    "    if not idxs: return tags\n",
    "    filtered = [corpus[i] for i in idxs]\n",
    "    vect = TfidfVectorizer(max_df=0.8, stop_words='english', ngram_range=(1,2))\n",
    "    X = vect.fit_transform(filtered)\n",
    "    feats = vect.get_feature_names_out()\n",
    "    for i, orig in enumerate(idxs):\n",
    "        arr = X[i].toarray().flatten()\n",
    "        top = arr.argsort()[::-1][:top_n]\n",
    "        tags[orig] = [feats[j] for j in top]\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f99b778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizer\n",
    "def summarize_purpose(text):\n",
    "    snippet = text[:1000]\n",
    "    out = summarizer(snippet, max_length=60, min_length=10, do_sample=False)\n",
    "    return out[0]['summary_text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c09a23c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 60, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 60, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 60, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 60, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 60, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 60, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 60, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 60, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Your max_length is set to 60, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n"
     ]
    }
   ],
   "source": [
    "#Pipeline\n",
    "texts_info = extract_texts(ROOT_PATH)\n",
    "# Build corpus for TF-IDF tagging\n",
    "corpus = [split_frontmatter(content)[1] if ext=='adoc' else content for _, content, ext in texts_info]\n",
    "tag_lists = extract_tags_tfidf(corpus)\n",
    "\n",
    "all_metadata = []\n",
    "for idx, (path, content, ext) in enumerate(texts_info):\n",
    "    front, body = split_frontmatter(content) if ext=='adoc' else ({}, content)\n",
    "    title = (extract_title(body) if ext=='adoc' else os.path.basename(path)) or os.path.basename(path)\n",
    "    purpose = summarize_purpose(body)\n",
    "    # Classification for doc_type/persona\n",
    "    doc_type = front.get('doc_type') or classifier(purpose, DOC_TYPE_LABELS)['labels'][0]\n",
    "    persona = front.get('persona') or classifier(purpose, PERSONA_LABELS)['labels'][0]\n",
    "    metadata = {\n",
    "        'source': path,\n",
    "        'file_type': ext,\n",
    "        'title': title,\n",
    "        'purpose': purpose,\n",
    "        'persona': persona,\n",
    "        'doc_type': doc_type,\n",
    "        'prerequisites': extract_prerequisites(body) if ext=='adoc' else [],\n",
    "        'difficulty': front.get('difficulty'),\n",
    "        'key_tasks': extract_headings(body, level=2) if ext=='adoc' else [],\n",
    "        'api_endpoints': extract_api_endpoints(body) if ext=='adoc' else [],\n",
    "        'reading_time_min': estimate_reading_time(body),\n",
    "        'version': front.get('version'),\n",
    "        'last_updated': front.get('last_updated'),\n",
    "        'tags': tag_lists[idx]\n",
    "    }\n",
    "    all_metadata.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7fd435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated metadata for 193 files (.adoc,yaml,yml,json). Written to metadata_catalog.json.\n"
     ]
    }
   ],
   "source": [
    "#Export JSON catalog\n",
    "with open('metadata_catalog.json', 'w', encoding='utf-8') as jf:\n",
    "    json.dump(all_metadata, jf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Generated metadata for {len(all_metadata)} files (.{','.join(EXTENSIONS)}). \" +\n",
    "      \"Written to metadata_catalog.json.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
